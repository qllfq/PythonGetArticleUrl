# -*- conding:utf-8 -*-

# 1.?????
# 2.????title?URL
# 3.??title???????URL???????
import requests
from lxml import etree
import time, random, xlwt

# class Doc_spider(object):
#
#     def __init__(self):
#         self.base_url = 'http://www.bjmda.com'
#         self.url = 'http://www.bjmda.com/Aboutus/ShowClass.asp?ClassID=12&page={}'
#         self.headers = {
#             'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.116 Safari/537.36'}
#
#     def get_request(self, url):
#         '''???????html'''
#         response = requests.get(url, headers=self.headers)
#         # time.sleep(random.random())
#         html = etree.HTML(response)
#         return html
#
#     def parse_page_html(self, html, url):
#         '''???????????title?URL'''
#
#         url_lists = html.xpath('//tr/td[2]/a[2]/@href')[1:]
#         temp_lists = html.xpath('//tr/td[2]/a[2]/text()')[1:]
#         title_lists = [title.rstrip() for title in temp_lists]
#
#         urls = []
#         titles = []
#
#         for i in range(len(title_lists)):
#             url = self.base_url + url_lists[i]
#             title = title_lists[i]
#             urls.append(url)
#             titles.append(title)
#
#         return urls, titles
#
#     def parse_detail(self, html):
#         '''????????,????????'''
#
#         lists = html.xpath("//td[@id='fontzoom']//tr")
#         content_list = []
#         for list in lists:
#             contents = list.xpath('.//td//text()')
#             new = []
#             for i in contents:
#                 new.append(''.join(i.split()))
#             content_list.append(new)
#
#         return content_list
#
#     def save_excel(self, sheet_name, contents, worksheet, workbook):
#         '''?????Excel'''
#
#         # ????workbook ????
#         #workbook = xlwt.Workbook()
#         # ????worksheet
#         #worksheet = workbook.add_sheet(sheet_name)
#
#         try:
#
#             for i in range(len(contents)):
#                 if len(contents[i+1])>1:
#                     content_list = contents[i + 1]
#
#                     # ??excel
#                     # ???? ?, ?, ?
#                     worksheet.write(i, 0, label=content_list[0])
#                     worksheet.write(i, 1, label=content_list[1])
#                     worksheet.write(i, 2, label=content_list[2])
#                     if len(contents[i+1])>3:
#                         worksheet.write(i, 3, label=content_list[3])
#
#                     # ??
#                     #workbook.save(sheet_name + '.xls')
#                     # time.sleep(0.1)
#         except:
#             print(sheet_name,'??OK')
#
#             pass
#
#     def run(self):
#         # 1.????????????
#         urls = [self.url.format(i + 1) for i in range(2)]
#
#         # ????workbook ????
#         workbook = xlwt.Workbook()
#
#         for url in urls:
#             html = self.get_request(url)
#             # 2.??????title?URL
#             list_urls, titles = self.parse_page_html(html, url)
#
#             for i in range(len(list_urls)):
#                 url_detail = list_urls[i]
#                 # ????????
#                 title_detail = titles[i]
#                 # 3.????????????????????
#                 html_detail = self.get_request(url_detail)
#                 # 4.?????????????
#                 contents = self.parse_detail(html_detail)
#                 # ???????????
#
#                 # ????worksheet
#                 worksheet = workbook.add_sheet(title_detail)
#                 self.save_excel(title_detail, contents,worksheet,workbook)
#         workbook.save('?????.xls')
#         print('????????')
def get_page():
    url = 'https://ieeexplore.ieee.org/document/6303953/keywords#keywords'
    headers = {
        'User-Agent':'https://ieeexplore.ieee.org/document/6303953/keywords#keywords'
    }
    response = requests.get(url,headers=headers)
    print(response.text)


if __name__ == '__main__':
    # doc = Doc_spider()
    # doc.run()
    get_page()